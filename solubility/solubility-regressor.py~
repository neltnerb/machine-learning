import tensorflow as tf
from rdkit import Chem
import csv
import itertools as it
import numpy as np
from pandas import *
import networkx as nx
import matplotlib.pyplot as plot
from sklearn import datasets, metrics, preprocessing, model_selection
import tensorflow.contrib.learn.python.learn as learn

sess = tf.InteractiveSession()

# First I need to import the raw data from the csv file.
#
# Note, I modified the original solubility.txt file to manually replace
# commas in compound names with semicolons so that the extractor doesn't
# confuse them for delimiters. I simultaneously parse the SMILES string
# into a rdkit molecule object for easier feature extraction.

y_data = []
SMILES_data = []
mol_data = []

# atom_data will contain a list of example molecules, with each member
# list indexed by the atom index and then with a number representing
# the atomic number of that atom.

ifile = open('solubility.txt', "rb")
reader = csv.reader(ifile, delimiter=',')

# This is the best way I could find to skip the first iterator. The
# method using islice requires that you know how many elements are in
# the iterator, which is definitely more complex than just using a
# flag.

isheader = True
maxAtoms = 0

for row in reader:
    if isheader:
        isheader = False
    else:
        y_data.append(float(row[1]))
        SMILES_data.append(row[3])
        molecule = Chem.MolFromSmiles(row[3])
        numAtoms = molecule.GetNumAtoms()
        if numAtoms > maxAtoms:
            maxAtoms = numAtoms
        mol_data.append(molecule)
        
# Manually set maxAtoms to help get the TensorFlow code right.
maxAtoms = 60
        
# Create numpy array for atom descriptors. Size is [n, i, m] where n
# is the number of examples, i is the max number of atoms in a molecule
# seen while parsing the data, and m is 11 to represent H, C, N, O, F,
# P, S, Cl, Br, I, or metal.

atom_data = np.zeros((len(mol_data), maxAtoms, 11), dtype = float)

# Create numpy array for bond descriptors. Size is [n, i, i, m] where n
# is the number of examples, i is the max number of atoms in a molecule,
# and m is 4 to represent single, double, triple, and aromatic bonds.

bond_data = np.zeros((len(mol_data), maxAtoms, maxAtoms, 4), dtype = float)

# Create numpy array for graph distance. Size is [n, i, i, 7] where n
# is the number of examples, i is the max number of atoms in a molecule,
# and m is 7 where the boolean array contains True if the distance from
# the first atom to the second is less than the bin index. So,
# [0, 0, 1, 3] means whether in training example 0, atoms 0 and 1 are
# closer than 3 bonds away at the shortest distance. These will be
# calculated by building an adjacency matrix from the bond_data array
# and using networkx to convert that to a graph representation which
# is easier to extract the shortest path lengths from.

example_index = 0
distance_data = np.zeros((len(mol_data), maxAtoms, maxAtoms, 7), dtype = float)
graph_data = []

for molecule in mol_data:
    for atom in molecule.GetAtoms():
        atomicNum = atom.GetAtomicNum()
        if atomicNum == 1:
            atom_data[example_index,atom.GetIdx(),0] = 1
        elif atomicNum == 6:
            atom_data[example_index,atom.GetIdx(),1] = 1
        elif atomicNum == 7:
            atom_data[example_index,atom.GetIdx(),2] = 1
        elif atomicNum == 8:
            atom_data[example_index,atom.GetIdx(),3] = 1
        elif atomicNum == 9:
            atom_data[example_index,atom.GetIdx(),4] = 1
        elif atomicNum == 15:
            atom_data[example_index,atom.GetIdx(),5] = 1
        elif atomicNum == 16:
            atom_data[example_index,atom.GetIdx(),6] = 1
        elif atomicNum == 17:
            atom_data[example_index,atom.GetIdx(),7] = 1
        elif atomicNum == 35:
            atom_data[example_index,atom.GetIdx(),8] = 1
        elif atomicNum == 53:
            atom_data[example_index,atom.GetIdx(),9] = 1
        else:
            atom_data[example_index,atom.GetIdx(),10] = 1
    adjacency = np.zeros((molecule.GetNumAtoms(), molecule.GetNumAtoms()), dtype = int)
    for bond in molecule.GetBonds():
        atom1 = bond.GetBeginAtomIdx()
        atom2 = bond.GetEndAtomIdx()
        bondtype = bond.GetBondTypeAsDouble()
        if bondtype == 1:
            bond_data[example_index,atom1,atom2,0] = 1
            bond_data[example_index,atom2,atom1,0] = 1
        elif bondtype == 1.5:
            bond_data[example_index,atom1,atom2,3] = 1
            bond_data[example_index,atom2,atom1,3] = 1
        elif bondtype == 2:
            bond_data[example_index,atom1,atom2,1] = 1
            bond_data[example_index,atom2,atom1,1] = 1
        elif bondtype == 3:
            bond_data[example_index,atom1,atom2,2] = 1
            bond_data[example_index,atom2,atom1,2] = 1
    for atom1 in range(0, molecule.GetNumAtoms()):
        for atom2 in range(atom1+1, molecule.GetNumAtoms()):
            if any(bond_data[example_index,atom1,atom2]):
                adjacency[atom1,atom2] = 1
                adjacency[atom2,atom1] = 1
    graph = nx.from_numpy_matrix(adjacency)
    graph_data.append(graph)
    for atom1 in range(0, molecule.GetNumAtoms()):
        lengths = nx.single_source_shortest_path_length(graph, atom1)
        for atom2 in range(atom1, molecule.GetNumAtoms()):
            distance = lengths[atom2]
            for threshold in range(0,7):
                if distance <= threshold:
                    distance_data[example_index,atom1,atom2,threshold] = 1
                    distance_data[example_index,atom2,atom1,threshold] = 1
    example_index += 1

# Uncomment below for checking data validity.

#while 1:
#    print "Which example to query?"
#    example = int(raw_input())
    
#    np.random.shuffle(X)
#    print atom_data
#    print DataFrame(atom_data[example,:10])
#    print DataFrame(bond_data[example,0,:10])
#    print DataFrame(distance_data[example,0,:,:])
#    nx.draw_networkx(graph_data[example])
#    plt.show()
    
# The actual machine learning part. First I will figure out how to implement
# the "weave" layers to fingerprint the molecules based on the above data.

# First step seems to be to convert the numpy arrays I created above into
# TensorFlow tensors. This seems to be okay, but I don't know how to test
# it other than knowing that it doesn't produce errors. The data is 4D and
# large, so viewing it would be quite hard.

# First I will flatten the input data and concatenate it along with y in
# the first column to produce an X vector with all examples, using float32
# to avoid compatibility issues. Y needs to be converted to an np.array for
# concatenation compatibility.

y_data = np.array(y_data)
y_data = y_data.reshape((y_data.shape[0], -1))
atom_data = atom_data.reshape((atom_data.shape[0], -1))
bond_data = bond_data.reshape((bond_data.shape[0], -1))
distance_data = distance_data.reshape((distance_data.shape[0], -1))

x = np.array(np.concatenate((y_data, atom_data, bond_data, distance_data), axis=1))
x = x.astype(np.float32)

# And then finally shuffle it and produce separate training, CV, and test
# datasets. I will use 60%, 20%, 20%.

np.random.shuffle(x)

n_train = int(0.6 * atom_data.shape[0])
n_CV = int(0.2 * atom_data.shape[0])

# For the test size, subtract to eliminate rounding issues.
n_test = int(atom_data.shape[0]-n_train-n_CV)

y_train = x[0:n_train,0]
x_train = x[0:n_train,1:-1]
y_CV = x[n_train:n_train+n_CV,0]
x_CV = x[n_train:n_train+n_CV,1:-1]
y_test = x[n_train+n_CV:-1,0]
x_test = x[n_train+n_CV:-1,1:-1]

# And finally convert to feature columns

features_train = learn.infer_real_valued_columns_from_input(x_train)
features_CV = learn.infer_real_valued_columns_from_input(x_CV)
features_test = learn.infer_real_valued_columns_from_input(x_test)

# Next we set up the regressor. This uses the much simplified approach
# of the learn contrib team. They have a lingo for creating custom
# regressors, so really that's probably the way for me to approach this
# to avoid as many errors as possible.

regressor = learn.DNNRegressor(feature_columns=features_train, hidden_units=[200,200,200,200,200,200,200], model_dir = 'model/')

# And this is the section that actually runs the regressor. Note that I
# no longer need to bother with things like figuring out batching or
# creating dummy variables in the main code body.

regressor.fit(x_train, y_train, steps=10000, batch_size=50)

# And let's make some predictions to get a score for the results.

y_predictions = list(regressor.predict(x_test, as_iterable=True))
score = metrics.mean_squared_error(y_predictions, y_test)

plot.scatter(y_predictions, list(y_test))
plot.xlabel("Predicted Solubility")
plot.ylabel("Real Solubility")
plot.title("Real vs Predicted Solubility")
plot.show()

print DataFrame([y_predictions[0:20], list(y_test[0:20])])
print ("MSE: %f" % score)
